# 勘误表

## 一刷
### P6

LSTM 通过引入门控机制解决了 RNN 中的梯度消 失和梯度爆炸问题，

解决改为缓解。

### P51

输出改为

 词汇表： ['Kage', 'Niuzong', 'Xiaoxue', 'Mazong', 'Xiaobing', 'Teacher', 'is', 'Boss', 'Student']
 词汇到索引的字典： {'Kage': 0, 'Niuzong': 1, 'Xiaoxue': 2, 'Mazong': 3, 'Xiaobing': 4, 'Teacher': 5, 'is': 6, 'Boss': 7, 'Student': 8}
 索引到词汇的字典： {0: 'Kage', 1: 'Niuzong', 2: 'Xiaoxue', 3: 'Mazong', 4: 'Xiaobing', 5: 'Teacher', 6: 'is', 7: 'Boss', 8: 'Student'}
 词汇表大小： 9

### P53
print("Skip-Gram 数据样例（已编码）：", [(one_hot_encoding(context, word_to_idx),            word_to_idx[target]) for context, target in skipgram_data[:3]])

改为

print("Skip-Gram数据样例（已编码）：", [(one_hot_encoding(target, word_to_idx), word_to_idx[context]) for context, target in skipgram_data[:3]])

### P53

输出改为

One-Hot 编码前的单词： Teacher
One-Hot 编码后的向量： tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.])
Skip-Gram数据样例（已编码）： [(tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.]), 6), (tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.]), 5), (tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.]), 0)]

这是因为前后编码要对齐 （P51和P53）

## P73

输出改为

 词汇表： {'爸爸': 0, '我': 1, '玩具': 2, '爱': 3, '挨打': 4, '喜欢': 5, '讨厌': 6}
 词汇表大小： 7

## P75

输出改为

 输入批处理数据： tensor([[1, 5], [1, 3]])
 输入批处理数据对应的原始词： [['我', '喜欢'], ['我', '爱']]
 目标批处理数据： tensor([2, 0])
 目标批处理数据对应的原始词： ['玩具', '爸爸']

这是因为前后编码要对齐 （P73和P75）
