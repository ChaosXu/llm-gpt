# 勘误表

## 一刷
### P6

LSTM 通过引入门控机制解决了 RNN 中的梯度消 失和梯度爆炸问题，

解决改为缓解。

### P51

输出改为

 词汇表： ['Kage', 'Niuzong', 'Xiaoxue', 'Mazong', 'Xiaobing', 'Teacher', 'is', 'Boss', 'Student']
 词汇到索引的字典： {'Kage': 0, 'Niuzong': 1, 'Xiaoxue': 2, 'Mazong': 3, 'Xiaobing': 4, 'Teacher': 5, 'is': 6, 'Boss': 7, 'Student': 8}
 索引到词汇的字典： {0: 'Kage', 1: 'Niuzong', 2: 'Xiaoxue', 3: 'Mazong', 4: 'Xiaobing', 5: 'Teacher', 6: 'is', 7: 'Boss', 8: 'Student'}
 词汇表大小： 9

### P53
print("Skip-Gram 数据样例（已编码）：", [(one_hot_encoding(context, word_to_idx),            word_to_idx[target]) for context, target in skipgram_data[:3]])

改为

print("Skip-Gram数据样例（已编码）：", [(one_hot_encoding(target, word_to_idx), word_to_idx[context]) for context, target in skipgram_data[:3]])

### P53

输出改为

One-Hot 编码前的单词： Teacher
One-Hot 编码后的向量： tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.])
Skip-Gram数据样例（已编码）： [(tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.]), 6), (tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.]), 5), (tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.]), 0)]

这是因为前后编码要对齐 （P51和P53）

### P73

输出改为

 词汇表： {'爸爸': 0, '我': 1, '玩具': 2, '爱': 3, '挨打': 4, '喜欢': 5, '讨厌': 6}
 词汇表大小： 7

### P74

输出改为

 输入批处理数据： tensor([[1, 5], [1, 3]])
 输入批处理数据对应的原始词： [['我', '喜欢'], ['我', '爱']]
 目标批处理数据： tensor([2, 0])
 目标批处理数据对应的原始词： ['玩具', '爸爸']

这是因为前后编码要对齐 （P73和P74）

## 二刷

### P1

零基础机器学习 改成 零基础学机器学习

### P60

原文：

我们使用 PyTorch 实现了一个简单的 Word2Vec（这里是 Skip Gram）模型。模型包括输入层、隐藏层和输出层。输入层接收<span style="color:red;">**周围词**</span>（以 One-Hot 编码后的向量形式表示）。接下来，输入层到隐藏层的权重矩阵（记为 input_to_hidden）将这个向量转换为词嵌入，该词嵌入直接作为隐藏层的输出。隐藏层到输出层的权重矩阵（记为 hidden_to_output）将隐藏层的输出转换为一个概率分布，用于预测与<span style="color:red;">**周围词相关的中心词**</span>（以索引形式表示）。通过最小化预测词和实际目标词之间的分类交叉熵损失，可以学习词嵌入向量。下图展示了这个流程。

改为：

我们使用 PyTorch 实现了一个简单的 Word2Vec（这里是 Skip Gram）模型。模型包括输入层、隐藏层和输出层。输入层接收<span style="color:green;">**中心词**</span>（以 One-Hot 编码后的向量形式表示）。接下来，输入层到隐藏层的权重矩阵（记为 input_to_hidden）将这个向量转换为词嵌入，该词嵌入直接作为隐藏层的输出。隐藏层到输出层的权重矩阵（记为 hidden_to_output）将隐藏层的输出转换为一个概率分布，用于预测与<span style="color:green;">**中心词相关的周围词**</span>（以索引形式表示）。通过最小化预测词和实际目标词之间的分类交叉熵损失，可以学习词嵌入向量。下图展示了这个流程。

**图**
图中“前缀”改为“前后缀”。

### P60

接收前时间步 改为 接收当前时间步


### P93 / P96 / P98
数据集的维度和网络结构前后不一致，以Notebook为准

![Alt text](images/8daf31eae3eb392efbb4624feb3c53d.jpg)
![Alt text](images/e4f961d4b53cd9f956d26784e39daa7.jpg)
![Alt text](images/c41f10da370ac2ff5a3dcd63a55db06.jpg)

### P115

attn_weights  =  F.softmax(raw_weights, dim=2) #  形 状 (batch_size,  seq_len1,  seq_len2)

改为

attn_weights  =  F.softmax(scaled_weights, dim=2) #  形 状 (batch_size,  seq_len1,  seq_len2)

### P129

新代码（删除冗余的参数num_heads）：

```
def combine_heads(tensor):
    batch_size, num_heads, seq_len, head_dim = tensor.size()
    feature_dim = num_heads * head_dim
    output = tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, feature_dim)
    return output# 形状 : (batch_size, seq_len, feature_dim)
attn_outputs = combine_heads(attn_outputs)  # 形状 (batch_size, seq_len, feature_dim) 
```

### P163

第二个维度信息后增加一行注释线，排版整齐
```
        #------------------------- 维度信息 --------------------------------
        # enc_outputs 的维度是 [batch_size, seq_len, embedding_dim] 
        # attn_weights 的维度是 [batch_size, n_heads, seq_len, seq_len]  
        # 将多头自注意力 outputs 输入位置前馈神经网络层
```

增加注释线
```
        #------------------------- 维度信息 --------------------------------
        # enc_outputs 的维度是 [batch_size, seq_len, embedding_dim] 
        # attn_weights 的维度是 [batch_size, n_heads, seq_len, seq_len]    
        #-----------------------------------------------------------------   
        # 将多头自注意力 outputs 输入位置前馈神经网络层
```

## 三刷

### P1

“人工通用智能”，改为 “通用人工智能”， 全书都要改。 “通用人工智能”是更让人接受的翻译。

### P16

“接收前时间步”，改为 “接收当前时间步”。

### P52


```
data.append((neighbor, word))
```
改为
```
data.append((word, neighbor))
```

### P53

每个数据包含两个张量（Tensor），前一个是输入（Input），格式是上下文词的One-Hot编码，后一个是目标（Target），格式则是目标词的索引”

改为

每个数据包含两个张量（Tensor），前一个是输入（Input），格式是中心词（中心词）的One-Hot编码，后一个是要预测的目标（Target），格式是上下文词的索引”

### P55


```
for context, target in skipgram_data:     
        X = one_hot_encoding(target, word_to_idx).float().unsqueeze(0) # 将中心词转换为 One-Hot 向量  
```
改为
```
for center_word, context in skipgram_data:     
        X = one_hot_encoding(center_word, word_to_idx).float().unsqueeze(0) # 将中心词转换为 One-Hot 向量  
```



### P65

第一段代码最好加上下面几行，意味着实例化并打印模型
```
embedding_size = 2 # 设定嵌入层的大小，这里选择 2 是为了方便展示
skipgram_model = SkipGram(voc_size, embedding_size)  # 实例化 Skip-Gram 模型
print("Skip-Gram 模型：", skipgram_model)
```

### P109

"此x1元素"改成"x1中第一个元素"

### P176

注释中的

dec_self_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, tgt_seq_len, src_seq_len]

改为

dec_self_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, tgt_seq_len, tgt_seq_len]


### P180

最后一段 “不过，这次训练效果不理想”，改成“不过，这次测试效果不理想”

### P232

删去下面一行，因为后来没展示它。

    "hi , how are you?",